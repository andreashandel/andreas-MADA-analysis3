---
title: "MADA Exercise flu data analysis"
author: "Andreas Handel"
date: "10/28/2021"
output: html_document
---

  
Data is being cleaned with the separate `processingscript.R` file.

This script loads the cleaned data, does a bit more processing, and then performs some ML analyses.


# Setup

```{r, message = FALSE}
#load needed packages. make sure they are installed.
library(broom) #for cleaning up output from lm()
library(here) #for data loading/saving
library(recipes)
library(parsnip)
library(dials)
library(tune)
library(yardstick)
library(rsample)
library(workflows)
library(rpart)
library(rpart.plot)
library(glmnet)
library(ranger)
library(doParallel) # for parallel computing 
library(vip)
```


Load the data.

```{r}
#Path to data. Note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","processeddata.rds")
#load data
mydata <- readRDS(data_location)
```


Set a seed for reproducibility.

```{r}
# for reproducibility
set.seed(123)
```


Split data into train and test, set up cross-validation.

```{r}
# split train and test, using the outcome as stratifier
data_split <- rsample::initial_split(mydata,strata = 'BodyTemp')

# Create data frames for the two sets:
train_data <- rsample::training(data_split)
test_data  <- rsample::testing(data_split)

# create CV object from training data
cv_data <- rsample::vfold_cv(train_data, v = 5, repeats = 5, strata = 'BodyTemp')
```



# Preprocessing

Create a recipe for the model fitting. We don't need to remove any NA or do imputation or standardization or anything else.
Therefore our recipe is fairly short, we just code all categorical variables as dummy variables.

```{r}
fit_recipe <- 
  recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal()) 
```


# Null model

For a **continuous outcome**, using RMSE as our performance metric, a null-model that doesn't use any predictor information is one that always just predicts the mean of the data. We'll compute the performance of such a "model" here. It's useful for comparison with the real models. We'll print both numbers here, and then compare with our model results below. Since our performance metric is RMSE, we compute that here with the "model prediction" always just being the mean of the outcomes.

```{r}
RMSE_null_train <- sqrt(sum( (train_data$BodyTemp - mean(train_data$BodyTemp))^2 )/nrow(train_data))
RMSE_null_test <- sqrt(sum( (test_data$BodyTemp - mean(test_data$BodyTemp))^2 )/nrow(test_data))
print(RMSE_null_train)
print(RMSE_null_test)
```



# Fit/tune different models

Now let's fit a few models.

## Tree model

### Tree model setup 

Starting with a tree-based model

```{r, start-tree}
# define the model
tree_model <-  decision_tree() %>% 
  set_args( cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")           
```

```{r}
#set workflow
tree_wf <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(fit_recipe)
```

### Tree model tuning

Define tune grid and do tuning. This might take a while. Note that there are many ways you can do the tuning with `tidymodels`.

Also note that I'm using parallel computing below. This makes things run faster, but can at times be iffy. If you get weird error messages, you might want to comment out each command that has to do with parallel running, namely the `makePSOCKcluster`, `registerDoParallel` and `stopCluster()` commands everywhere. This just means your code will run longer.

```{r, tune-tree}
#for parallel computing
#makes things faster. If not wanted, can be commented out, together with last line of this block.
ncores = 18 #adjust based on your computer. 
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

#tuning grid
tree_grid <- dials::grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)
#tune the model
tree_tune_res <- tree_wf %>% 
  tune::tune_grid(
    resamples = cv_data,
    grid = tree_grid,
    metrics = yardstick::metric_set(rmse) 
  )
# turn off parallel cluster
stopCluster(cl)
```


### Tree model evaluation

Now that tuning is done, look at the tuning process a bit. This plots model performance during tuning.

```{r}
#see a plot of performance for different tuning parameters
tree_tune_res %>% autoplot()
```

Getting the model that was determined to be best (via cross-validation) by tuning.

```{r}
# get the tuned model that performs best 
best_tree <- tree_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_tree_wf <- tree_wf %>% finalize_workflow(best_tree)
# fitting best performing model
best_tree_fit <- best_tree_wf %>% 
                 fit(data = train_data)
#predicting outcomes for final model
tree_pred <- predict(best_tree_fit, train_data)
```


Plotting final tree.

```{r}
rpart.plot(extract_fit_parsnip(best_tree_fit)$fit)
```


This is a bad tree. I got a different tree with a bit more tuning (though an overall not much better model).


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(tree_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(tree_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

We can see that the model only predicts 2 different values. Not a great model.


Looking at model performance. I think it would be more intuitive to get the performance from the `best_tree_fit` object, but I can't figure out how to do that.

```{r}
tree_perfomance <- tree_tune_res %>% show_best(n = 1)
print(tree_perfomance)
```

Comparing the RMSE to the null model, we see that it is not much better. 

Based on our model evaluation, I think we can safely say here that a tree-based model is no good.


## LASSO linear model

Repeating the steps above, now for LASSO.


### LASSO setup


```{r, start-lasso}
#model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) #mixture = 1 means we use the LASSO model

#workflow
lasso_wf <- workflow() %>%
  add_model(lasso_model) %>% 
  add_recipe(fit_recipe)
```



### LASSO tuning

```{r, tune-lasso}
#parallel computing
ncores = 18 #adjust based on your computer
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)


#tuning grid
lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#tune model
lasso_tune_res <- lasso_wf %>% 
  tune_grid(resamples = cv_data,
            grid = lasso_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse)
            )
# turn off parallel cluster
stopCluster(cl)
```


### LASSO evaluation

```{r}
#see a plot of performance for different tuning parameters
lasso_tune_res %>% autoplot()
```




```{r}
# get the tuned model that performs best 
best_lasso <- lasso_tune_res %>%  select_best(metric = "rmse")

# finalize workflow with best model
best_lasso_wf <- lasso_wf %>% finalize_workflow(best_lasso)

# fitting best performing model
best_lasso_fit <- best_lasso_wf %>% 
  fit(data = train_data)
lasso_pred <- predict(best_lasso_fit, train_data)
```

Plotting LASSO variables as function of tuning parameter


```{r}
x <- best_lasso_fit$fit$fit$fit
plot(x, "lambda")
```

As one sees, the larger the regularization penalty, the fewer predictor variables that remain in the model. (Once a coefficient is at 0, the corresponding variable is not in the model anymore).


This shows the variables that are part of the best-fit LASSO model, i.e. those that have a non-zero coefficient.

```{r}
tidy(extract_fit_parsnip(best_lasso_fit)) %>% filter(estimate != 0)
```


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(lasso_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(lasso_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

The diagnostic plots show that this model isn't much better either. We want the points to be along the red lines in each plot. They are not.



Looking at model performance. 

```{r}
lasso_perfomance <- lasso_tune_res %>% show_best(n = 1)
print(lasso_perfomance)
```

A somewhat lower RMSE, so a bit better performance. So seems overall that the LASSO model is not quite as bad as the tree, but I wouldn't call it a good model. The observed/predicted and residual plots don't look great, and the performance is not much better. Let's see if we have more luck with the random forest.



## Random forest model

Repeating the steps above, now for a random forest.


### Random forest setup


```{r, start-rf}
rf_model <- rand_forest() %>%
  set_args(mtry = tune(),     
    trees = tune(),
    min_n = tune()
  ) %>%
  # select the engine/package that underlies the model
  set_engine("ranger",
             num.threads = 18, #for some reason for RF, we need to set this in the engine too
             importance = "permutation") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("regression")           
```


```{r}
#workflow
rf_wf <- workflow() %>%
  add_model(rf_model) %>% 
  add_recipe(fit_recipe)
```


### Random forest tuning

```{r, tune-rf}
#parallel computing
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

#tuning grid
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )

# tune the model, optimizing RMSE
rf_tune_res <- rf_wf %>%
  tune_grid(
            resamples = cv_data, #CV object
            grid = rf_grid, # grid of values to try
            metrics = metric_set(rmse) 
  )
# turn off parallel cluster
stopCluster(cl)
```


### Random forest evaluation

```{r}
#see a plot of performance for different tuning parameters
rf_tune_res %>% autoplot()
```

Based on the tuning plot, it might be worth doing more tuning, e.g. with higher minimum node size. Not doing that here, I don't think the model performance will improve a lot. (Though for a real research problem, I would explore more).


```{r}
# get the tuned model that performs best 
best_rf <- rf_tune_res %>%  select_best(metric = "rmse")

# finalize workflow with best model
best_rf_wf <- rf_wf %>% finalize_workflow(best_rf)

# fitting best performing model
best_rf_fit <- best_rf_wf %>% 
  fit(data = train_data)
rf_pred <- predict(best_rf_fit, train_data)
```


For random forest models, one can't easily look at the final model. One can however look at the most important predictors for the final model.

```{r}
#pull out the fit object
x <- best_rf_fit$fit$fit$fit
#plot variable importance
vip::vip(x, num_features = 20)
```


Subjective fever being the most important/predictive variable for body temperature makes sense. So some internal consistency, which is good. Note that a RF model does not kick out any variables, all stay in, but some are more important than others.



Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(rf_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(rf_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```


Looking at model performance. 

```{r}
rf_perfomance <- rf_tune_res %>% show_best(n = 1)
print(rf_perfomance)
```

Based on the diagnostic plots and the model performance, it seems that RF isn't much better.



# Picking a Model 

So none of these models are actually any good. Overall, we should conclude that - at least with the models tried so far - the predictor variable information is not very useful at predicting the outcome. 

But for the sake of this exercise, we'll pick one model and apply it to the test data. Since RF and LASSO perform about the same, I'll go with the simpler LASSO. So let's give that model a final check.


# Final Model Evaluation

We'll now apply the model a single time to the test data.

```{r}
# for reasons that make no sense (likely a bug in tidymodels)
# I need to re-start a parallel cluster here to get the command below to work 
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
# fit on the training set and evaluate on test set
final_fit <- best_lasso_wf  %>% last_fit(data_split)
stopCluster(cl)
```

Let's look at the performance of the final fit, evaluated on the test data.

```{r}
test_performance <- final_fit %>% collect_metrics()
print(test_performance)
```

If we compare the RMSE for the test data to the RMSE on the training data, we see it's similar. That's good, it suggests we might have avoided overfitting. In this case, since none of the models fit well anyway, the risk of overfitting wasn't really present.

If we compare the RMSE on the test data to the performance/RMSE of the null model on the test data, we see it's not much better. That's bad. It shows again that none of these models are good.


And just another look at the diagnostic plots for the test data.

```{r}
test_predictions <- final_fit %>% collect_predictions()
```


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(test_predictions$.pred,test_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(test_predictions$.pred-test_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

Still bad.



# Overall conclusion

I didn't know this was going to happen, but it turns out that at least among the models I tried, and the amount of tuning I did, I didn't find any that worked well. If we tried some other models, maybe there is one that predicts well. Or if we do some more processing/feature engineering, or better tuning, we might get better performance. 

More likely is that the data we have is just not very predictive of the outcome. That happens, sometimes your data is just not good enough to say much. Having checks, such as diagnostic plots and comparison to null models is important to see if your model is good. 

Of course for any real model, you should dig even deeper. If no model performs well, try to figure out why. Once a model seems to perform well, there are still things that can be wrong, so carefully "poking" it until you fully understand the model is important.

It could be that with this data, being able to predict the categorical outcome of `Nausea` might work. Or maybe not. I haven't tried.


